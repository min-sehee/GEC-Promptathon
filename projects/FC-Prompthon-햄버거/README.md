# 햄버거🍔

## 팀원 소개

1) 한지수 (인공지능학과 23학번) 
2) 김서정 (인공지능학과 23학번) 
3) 민세희 (인공지능학과 22학번) 
4) 박채연 (언더우드학부 경제학과 22학번)

## 프롬프트 템플릿 소개 및 활용한 테크닉

본 프로젝트는 한국어 문장 교정(Grammar Error Correction)을 위해  
**전문가(Specialist) → 검토자(Reviewer)** 구조의 **2-Step Prompting 전략**을 사용했습니다.

### 🔹 1) 1차 교정(Specialist Prompt)
- 역할 기반(Persona-based) 프롬프트로 **전문 한국어 교정가**의 행동을 부여했습니다.
- 맞춤법, 띄어쓰기, 조사, 문장부호, 문법 오류를 **최대한 많이(High Recall)** 찾도록 유도했습니다.
- 13개의 다양한 오류 예시(조사·사이시옷·어휘 선택·단위·오탈자 등)를 제공하여  
  모델이 다양한 교정 패턴을 학습하도록 했습니다.

### 🔹 2) 2차 검토(Reviewer Prompt)
- “명백한 오류만 수정한다”는 **Minimal Editing 전략**을 적용했습니다.
- 1차 교정본이 이미 자연스러우면 어떤 수정도 하지 않도록 강하게 제한해  
  **불필요한 과교정(over-correction)**을 방지했습니다.

### 🔹 3) 2-Call API 전략
- 각 문장은 **총 2번**의 API 호출로 처리됩니다.  
  (1차 교정 → 2차 검토)
- `calls_per_case = 2` 형태로 템플릿에 명시했습니다.
- 이를 통해 Recall 중심 교정에서도 Precision을 일정 수준 유지할 수 있었습니다.

### 🔹 4) Temperature & Output Control
- 모델의 일관성과 안정성을 위해 `temperature = 0.0`으로 고정했습니다.
- 시스템 프롬프트에서 “설명 없이 교정된 문장만 출력” 규칙을 명확히 명시해  
  불필요한 문답이나 누락을 방지했습니다.

### 🔹 5) 예시 선정 방식 (Dataset-driven Prompt Engineering)
스페셜리스트 프롬프트 예시는 임의로 구성한 것이 아니라,  
`train_dataset.csv`의 문법 오류 분포를 분석하여 **데이터 기반으로 선정한 예시 집합**입니다.

- 전체 오류 유형을 빈도수 기준으로 정렬한 뒤  
  **상위 5개 오류 유형에는 예시 2개씩**,  
  그다음으로 빈도가 높은 **3개 오류 유형에는 예시 1개씩**을 구성했습니다.
- 이를 통해  
  1) **고빈도 오류에 대한 Recall을 극대화**하고  
  2) 저빈도 유형도 하나씩 포함하여 **다양성(coverage)**을 확보했습니다.
  
즉, 단순히 예시를 많이 넣는 것이 아니라  
**실제 데이터 분포를 반영한 균형 잡힌 예시 구성**으로 프롬프트 효과를 최적화했습니다.

### 🔹 6) 2-Step 프롬프트 구조의 전략적 구성 (High Recall → High Precision 균형)
본 프로젝트의 2-Step 구조는 단순한 단계 분리가 아니라,  
**아주 의도적으로 설계된 “공격적 + 보수적 조합 전략”**입니다.

- **① 1차 Specialist Prompt → High Recall, Aggressive Editing**
  - 가능한 모든 오류를 찾는 것을 목표로 하는 공격적 교정 단계입니다.
  - 맞춤법·띄어쓰기·조사·어휘 선택 등 다양한 오류 유형을 적극적으로 탐지하여  
    *오류 발견률(Recall)*을 극대화하도록 설계했습니다.

- **② 2차 Reviewer Prompt → Low Recall, Conservative Editing**
  - 1차 교정이 과도한 경우를 억제하고, 명백한 오류만 최소한으로 수정하는 보수적 단계입니다.
  - 이를 통해 불필요한 변경(over-correction)을 방지해  
    *정확도(Precision)*를 유지하도록 했습니다.

이 두 단계를 결합하면  
**“공격적(High Recall) 1차 교정” + “보수적(High Precision) 2차 검토”**  
구조가 만들어지며, 이는 실제 GEC 연구에서도 널리 사용되는  
**precision–recall trade-off 최적화 전략**입니다.

추가적으로, 본 구조는 2-Call로 충분히 안정적인 품질을 확보하는  
경량(Lightweight) GEC Pipeline으로 설계되어 비용 효율도 함께 고려했습니다.

즉, 본 프로젝트는 단순한 프롬프트 조합이 아니라  
**GEC 연구에서 검증된 2-Step Pipeline을 그대로 프롬프트 레벨에서 구현**한 구조입니다.

또한 본 2-Step 구조는 최근 LLM 연구에서 널리 활용되는 
**Reflection Prompting(Self-Refine) 패턴과 동일한 방식으로 작동**하며,
1차 출력에 대한 자체 검토 단계를 추가함으로써 안정성과 신뢰도를 더욱 높였습니다.


---

## 프롬프트 템플릿 전문
최종 프롬프트 전문은  
👉 `prompt_template.json` 파일에 포함되어 있습니다.  
(specialist_prompt + reviewer_prompt + system prompt 전체)

---

## 제출 방법
프로젝트 제출은 Fork & Pull Request 방식을 따릅니다. 제출에 관한 상세 절차는 운영진이 별도로 안내한 "최종 프로젝트 제출 가이드" 문서를 참고해주세요.